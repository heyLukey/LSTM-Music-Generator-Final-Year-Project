{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dissertation Code - LSTM Music Generation System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import project dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import os\n",
    "from music21 import converter, note, chord, instrument, stream, duration, scale, interval, pitch\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Activation\n",
    "from keras.layers import BatchNormalization as BatchNorm\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from matplotlib import pyplot\n",
    "import pickle\n",
    "from keras.models import load_model\n",
    "import fluidsynth\n",
    "import subprocess\n",
    "import random\n",
    "\n",
    "print(\"MODULES LOADED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initialise some dictionaries\n",
    "\"\"\"\n",
    "note2inter = {\"C\": 0,\n",
    "              \"C#\": 1,\n",
    "              \"D\": 2,\n",
    "              \"E-\": 3,\n",
    "              \"E\": 4, \n",
    "              \"F\": 5, \n",
    "              \"F#\": 6, \n",
    "              \"G\": 7, \n",
    "              \"G#\": 8, \n",
    "              \"A\": 9,\n",
    "              \"B-\": 10,\n",
    "              \"B\": 11}\n",
    "\n",
    "inter2note = {0: \"C\",\n",
    "              1: \"C#\",\n",
    "              2: \"D\",\n",
    "              3: \"E-\",\n",
    "              4: \"E\",\n",
    "              5: \"F\",\n",
    "              6: \"F#\",\n",
    "              7: \"G\",\n",
    "              8: \"G#\",\n",
    "              9: \"A\",\n",
    "              10: \"B-\",\n",
    "              11: \"B\"}\n",
    "\n",
    "dur2string = {0.25: \"1/16\", \n",
    "              0.5: \"1/8\", \n",
    "              1.0: \"1/4\", \n",
    "              2.0: \"1/2\"}\n",
    "\n",
    "dur2float = {\"1/16\" : 0.25,\n",
    "             \"1/8\" : 0.5,\n",
    "             \"1/4\" : 1.0,\n",
    "             \"1/2\" : 2.0}\n",
    "\n",
    "\"\"\"\n",
    "This function takes in an array/list and a value.\n",
    "It compares this value to the elements in the array ,\n",
    "and returns the element closest to the given value (floored).\n",
    "\"\"\"\n",
    "def find_nearest(array, value):\n",
    "    \n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    \n",
    "    return array[idx]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This function creates an integer representation of a given scale\n",
    "The user must input a base note (sharps instead of flats),\n",
    "and whether they wish the scale to be minor or major\n",
    "\"\"\"\n",
    "def scale_create():\n",
    "    \n",
    "    p = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    key = note2inter[str(input(\"what key?: \")).upper()]\n",
    "    m_or_m = str(input(\"Minor or Major?: \"))\n",
    "    major = [p[key], p[key+2], p[key+4], p[key+5], p[key+7], p[key+9], p[key+11]]\n",
    "    minor = [p[key], p[key+2], p[key+3], p[key+5], p[key+7], p[key+8], p[key+10]]\n",
    "    \n",
    "    if m_or_m.lower() == \"major\":\n",
    "        scale = major\n",
    "    elif m_or_m.lower() == \"minor\":\n",
    "        scale = minor\n",
    "    else:\n",
    "        print(\"Please enter a valid key\")\n",
    "        \n",
    "    return scale\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This function takes in an array and a music21 note object.\n",
    "The array should be an integer representation of a major or minor scale.\n",
    "The function will then transpose the given note to fit the given scale.\n",
    "The transposed note is returned.\n",
    "\"\"\"\n",
    "def scale_notes(scale, note):\n",
    "    tmp = re.split('(\\d+)',note)\n",
    "    before, octave = tmp[0], tmp[1]\n",
    "    transpose = note2inter[before]\n",
    "    final = inter2note[find_nearest(scale, transpose)] + octave\n",
    "    return final\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This function takes in an array and a music21 chord object.\n",
    "The array should be an integer representation of a major or minor scale.\n",
    "The function will then transpose the notes in the chord to fit the given scale.\n",
    "The transposed chord is returned.\n",
    "\"\"\"\n",
    "def scale_chords(scale, chord):\n",
    "    transposed = []\n",
    "    for note in chord.normalOrder:\n",
    "        transposed.append(find_nearest(scale, note))\n",
    "        \n",
    "    if(len(set(transposed)) == 1):\n",
    "        final = inter2note[(transposed[0])] + \"4\"\n",
    "    else:\n",
    "        final = '.'.join(str(n) for n in transposed)\n",
    "        \n",
    "    return final\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This function parses all of the MIDI files in a given directory and transcribes their information to an array\n",
    "All notes and chords are transposed to a specified scale given by user input\n",
    "All durations are changed to the nearest 1/32, 1/16, 1/8, etc.\n",
    "\"\"\"\n",
    "def midi_parse(DATADIR):\n",
    "    \n",
    "    print(\"--- PARSING MIDI ---\")\n",
    "    file_num, notes = 0, [] \n",
    "    scale = scale_create()\n",
    "    lengths = np.array([0.25, 0.5, 1, 2])\n",
    "    files = [os.path.join(DATADIR,fle) for fle in os.listdir(DATADIR) if fle.endswith(\".mid\")]\n",
    "    random.shuffle(files)\n",
    "    \n",
    "    for file in tqdm(files, \"PARSING\"):\n",
    "        \n",
    "        file_num += 1\n",
    "        midi_notes, current_parse = [], []\n",
    "        midi_stream = converter.parseFile(os.path.join(DATADIR, file)) # Convert MIDI file to music21 stream\n",
    "        piano_parts = []\n",
    "        instr = instrument.Piano\n",
    "        \n",
    "        # Flatten multi-track MIDI\n",
    "        try:\n",
    "            for part in instrument.partitionByInstrument(midi_stream):\n",
    "                if isinstance(part.getInstrument(), instr):\n",
    "                    current_parse = part[0].recurse()\n",
    "        except:\n",
    "            current_parse = midi_stream.flat.notes\n",
    "    \n",
    "        # Go through the music21 stream and append the objects to a usable array\n",
    "        for element in current_parse:\n",
    "            if isinstance(element, note.Note): \n",
    "                check = element.duration.quarterLength\n",
    "                dur = dur2string[find_nearest(lengths, check)]\n",
    "                scaled = scale_notes(scale, str(element.pitch))\n",
    "                midi_notes.append((scaled)+ \"_\" + str(dur))\n",
    "                \n",
    "            if isinstance(element, chord.Chord):                 \n",
    "                check = element.duration.quarterLength\n",
    "                dur = dur2string[find_nearest(lengths, check)]\n",
    "                scaled = scale_chords(scale, element)\n",
    "                midi_notes.append(scaled+ \"_\" +str(dur))\n",
    "                \n",
    "            elif isinstance(element, note.Rest):                \n",
    "                check = element.duration.quarterLength\n",
    "                dur = dur2string[find_nearest(lengths, check)]\n",
    "                midi_notes.append((\"rest\")+ \"_\" +str(dur))\n",
    "\n",
    "        notes.extend(midi_notes) # Add objects from this file to overall array of notes \n",
    "        print(\"FILE {:3}\".format(str(file_num)), end=\"\")\n",
    "        print(\" =   {:100} \".format(file), end=\"\")\n",
    "        print(\"NOTES: \" + str(len(midi_notes)))\n",
    "        \n",
    "    print(\"\\nTOTAL FILES : \" + str(file_num))\n",
    "    print(\"TOTAL NOTES : \" + str(len(notes)))\n",
    "    \n",
    "    return notes\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This function takes an array of notes and encodes them for use in a nueral network.\n",
    "\"\"\"\n",
    "def midi_encode(notes, note_info, seq_len):\n",
    "    print(\"--- ENCODING MIDI ---\")\n",
    "    note_set, n_vocab = note_info[0], note_info[1]\n",
    "    seq_in, seq_out = [], []\n",
    "    X_network, y_network = [], []\n",
    "    note2int = dict([(note, intt) for intt, note in enumerate(sorted(note_set))]) # Create dict to map note to int\n",
    "    \n",
    "    for i in tqdm(range(0, len(notes) - seq_len), desc=\"ENCODING\"):\n",
    "        seq_in = notes[i:i + seq_len]  # The 100 notes before the output note\n",
    "        seq_out = notes[i + seq_len]  # The output note\n",
    "        X_network.append([note2int[unmapped] for unmapped in seq_in])  # Map each input to int and append\n",
    "        y_network.append(note2int[seq_out])  # Map output to int and append\n",
    "    \n",
    "    X_network = np.reshape(X_network, (len(X_network), seq_len, 1))\n",
    "    X_network = X_network / float(n_vocab) # Normalise input\n",
    "    \n",
    "    X_network, y_network = shuffle(X_network, y_network) # Shuffle Data\n",
    "    y_network = to_categorical(y_network) # Onehot encode\n",
    "    \n",
    "    return X_network, y_network\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This function creats an LSTM model and saves it to a specified file.\n",
    "\"\"\"\n",
    "def model_create(notes, note_info, X_network, y_network, model_name):\n",
    "    print(\"--- CREATING MODEL ---\\n\")\n",
    "    note_set, n_vocab = note_info[0], note_info[1]\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, input_shape=(X_network.shape[1], X_network.shape[2]), return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(512, return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(512))\n",
    "    model.add(Dense(512))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(n_vocab))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.save(model_name)\n",
    "    print(\"MODEL CREATED AT: Model_Data\\\\\" +model_name+ \"\\n\")\n",
    "               \n",
    "    return\n",
    "\n",
    "\"\"\"\n",
    "This function trains a model using encoded data.\n",
    "The weights are all saved to the weights filepath.\n",
    "\"\"\"\n",
    "def model_train(X_network, y_network, model_name, epoch):\n",
    "    print(\"--- TRAINING MODEL ---\\n\")\n",
    "    filepath = \"Model_Data\\\\Weights\\\\{epoch:02d}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "    model = load_model(model_name)\n",
    "    history = model.fit(X_network, y_network, epochs=epoch, batch_size=64, callbacks=[checkpoint], validation_split=0.2)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "The function takes an array of notes and turns it into a music21 stream. \n",
    "This music21 stream is then written to a MIDI file.\n",
    "\"\"\"\n",
    "def stream2midi(y_prediction, identifier, MIDIDIR):\n",
    "    \n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "    \n",
    "    for element in y_prediction:   \n",
    "        pitch, dur = element.split(\"_\") # Split the string into its object and duration\n",
    "        dur = dur2float[dur] # Use dict to get quarterlength representation of string\n",
    "        \n",
    "        # The element is a chord\n",
    "        if('.' in pitch) or pitch.isdigit():\n",
    "            notes_in_chord = pitch.split('.') # Split string into induvidual notes in chord\n",
    "            notes = []\n",
    "            # Iterate though the chords notes and add them to a chord object\n",
    "            for current_note in notes_in_chord: \n",
    "                new_note = note.Note(int(current_note))\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.duration = duration.Duration(dur)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord) \n",
    "            \n",
    "        # The element is a rest\n",
    "        elif pitch == \"rest\":\n",
    "            new_rest = note.Rest()\n",
    "            new_rest.duration = duration.Duration(dur)\n",
    "            new_note.offset = offset  \n",
    "            output_notes.append(new_rest) \n",
    "            \n",
    "        # The element is a single note\n",
    "        else:\n",
    "            new_note = note.Note(pitch)\n",
    "            new_note.duration = duration.Duration(dur)\n",
    "            new_note.offset = offset  \n",
    "            new_note.storedInstrument = instrument.Piano()  \n",
    "            output_notes.append(new_note) \n",
    "        \n",
    "        offset += dur # Increase offset of next element by duration of current element\n",
    "\n",
    "    midi_stream = stream.Stream(output_notes) # Create the music21 stream\n",
    "    midi_stream.write('midi', fp=MIDIDIR+ \"\\\\\" +str(identifier)+ \".mid\") # Write the stream to MIDI\n",
    "    print(\"NOTES HAVE BEEN WRITTEN TO MIDI FILE AT: \" +MIDIDIR+ \"\\\\\" +str(identifier)+ \".mid\\n\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This function predicts a sequence of notes/chords/rests.\n",
    "To do this it uses a saved version of the model created earlier\n",
    "as well as specified weights.\n",
    "\"\"\"\n",
    "def model_predict(notes, note_info, X_network, model_name, weights, MIDIDIR):\n",
    "    \n",
    "    print(\"--- PREDICTING NOTES ---\\n\")\n",
    "    number2gen = int(input(\"How many MIDI files do you want to generate? \"))\n",
    "    note_set, n_vocab = note_info[0], note_info[1]\n",
    "    int2note = dict([(intt, note) for intt, note in enumerate(sorted(note_set))]) # Create dict for int -> note\n",
    "    \n",
    "    # Load the Model.\n",
    "    model_predict = load_model(model_name)\n",
    "    model_predict.load_weights(weights)\n",
    "    \n",
    "    # Create a specified number of \"predictions\".\n",
    "    for i in range(number2gen):     \n",
    "        start = np.random.randint(0, len(X_network)-1) # Decide where to start generating our music.\n",
    "        pattern = np.reshape(X_network[start], len(X_network[start])) # Take 100 notes from the 'start' index.\n",
    "        y_prediction = [] # Initialise output.\n",
    "\n",
    "        for note_index in tqdm(range(100), desc=\"FILE: \" +str(i+1)+ \" (index \"+str(start)+\")\"):\n",
    "            X_predict = np.reshape(pattern, (1, len(pattern), 1)) # Reshape pattern so it can be used in the model.\n",
    "            prediction = model_predict.predict(X_predict, verbose=0) # Predict notes using model.\n",
    "            # Get most likey note (highest int) from the mapping dictionary.\n",
    "            index = np.argmax(prediction)\n",
    "            result = int2note[index]\n",
    "            y_prediction.append(result)\n",
    "            # Reconfigure network input for next iteration.\n",
    "            pattern = np.insert(pattern, len(pattern), (index/ float(n_vocab)))\n",
    "            pattern = pattern[1:len(pattern)]\n",
    "        \n",
    "        stream2midi(y_prediction, start, MIDIDIR) # Turn the prediction into a MIDI file\n",
    "        \n",
    "\"\"\"\n",
    "This function converts midi files into listenable wav files.\n",
    "It does this by taking MIDI files at a specified directory,\n",
    "plays them using a specified soundfont, \n",
    "and renders that to an audio file (.wav)\n",
    "\"\"\"\n",
    "def midi2wav(in_dir, out_dir, soundfont):\n",
    "    \n",
    "    print(\"--- CONVERTING MIDI TO WAV ---\\n\")\n",
    "    \n",
    "    # Loop through directory containing MIDI files\n",
    "    for in_file in os.listdir(in_dir):\n",
    "        identifier = in_file.replace(\".mid\", \"\") # Extract name of MIDI file\n",
    "        out_file = str(identifier)+ \".wav\"\n",
    "        midi_dir = os.path.join(in_dir, in_file)\n",
    "        wav_dir = os.path.join(out_dir, out_file)\n",
    "        subprocess.call(['fluidsynth', '-F', wav_dir, soundfont, midi_dir]) # Call command to render audio using FluidSynth.\n",
    "        \n",
    "    print(\"MIDI HAS BEEN CONVERT TO WAV AT: \" +str(out_dir)+ \"\\n\")\n",
    "    \n",
    "\n",
    "print(\"FUNCTIONS LOADED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsystem 1 - Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell handles model creation, as described in section 4.6.2.\n",
    "\"\"\"\n",
    "\n",
    "def ss1_model_creation(DATASET_DIR):\n",
    "    \n",
    "    # 1. Parse data\n",
    "    notes = midi_parse(DATADIR)\n",
    "    note_set = set(notes) # Get rid of dupes\n",
    "    n_vocab = len(note_set) # The number of unique notes\n",
    "    note_info = (note_set, n_vocab) # Save info in tuple\n",
    "    \n",
    "    # 2. Encode data\n",
    "    X_network, y_network = midi_encode(notes, note_info, 100)\n",
    "    \n",
    "    # 3. Create model\n",
    "    model_name = \"Model_Data\\\\\" + str(input(\"ENTER MODEL NAME: \")) + \".h5\"\n",
    "    model_create(notes, note_info, X_network, y_network, model_name)\n",
    "    \n",
    "    # 4. Train model\n",
    "    epoch = int(input(\"ENTER NUMBER OF EPOCHS TO TRAIN FOR: \"))\n",
    "    model_train(X_network, y_network, model_name, epoch)\n",
    "    \n",
    "    \"\"\"\n",
    "    Serialization\n",
    "    \"\"\"\n",
    "    pickle_out = open(\"Pickled_Data\\\\notes.pickle\", \"wb\")\n",
    "    pickle.dump(notes, pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "    pickle_out = open(\"Pickled_Data\\\\X_network.pickle\", \"wb\")\n",
    "    pickle.dump(X_network, pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "    pickle_out = open(\"Pickled_Data\\\\y_network.pickle\", \"wb\")\n",
    "    pickle.dump(y_network, pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "    pickle_out = open(\"Model_Data\\\\model_name.pickle\", \"wb\")\n",
    "    pickle.dump(model_name, pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "    print(\"--- RELEVANT DATA HAS BEEN PICKLED ---\")\n",
    "    print(\"---           FINISHED             ---\")\n",
    "    \n",
    "    \n",
    "    \n",
    "DATADIR = \"C:\\\\Users\\\\Lukey\\\\Music_Project_Test\\\\Dataset\"\n",
    "ss1_model_creation(DATADIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsytem 2 - Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell handles music generation, as described in section 4.6.3.\n",
    "\"\"\"\n",
    "\n",
    "def ss2_generation(MIDIDIR, WAVDIR, SFDIR):\n",
    "    \n",
    "    \"\"\"\n",
    "    LOAD UP VALUES\n",
    "    \"\"\"\n",
    "    pickle_in = open(\"Pickled_Data\\\\notes.pickle\", \"rb\")\n",
    "    notes = pickle.load(pickle_in)\n",
    "    note_set = set(notes) # Get rid of dupes\n",
    "    n_vocab = len(note_set) # The number of unique notes\n",
    "    note_info = (note_set, n_vocab) # Save info in tuple\n",
    "    \n",
    "    pickle_in = open(\"Pickled_Data\\\\X_network.pickle\", \"rb\")\n",
    "    X_network = pickle.load(pickle_in)\n",
    "    \n",
    "    pickle_in = open(\"Model_Data\\\\model_name.pickle\", \"rb\")\n",
    "    model_name = pickle.load(pickle_in)\n",
    "    \n",
    "    # Specify weights to use (look at files for specific value)\n",
    "    weight_epoch = input(\"Using weights created at epoch: \")\n",
    "    weights = \"Model_Data\\\\Weights\\\\\" + weight_epoch + \".hdf5\" \n",
    "    \n",
    "    # 1. Predict Sequences\n",
    "    # 2. Create MIDI\n",
    "    model_predict(notes, note_info, X_network, model_name, weights, MIDIDIR)\n",
    "    \n",
    "    # 3. Render Audio\n",
    "    midi2wav(MIDIDIR, WAVDIR, SFDIR)\n",
    "\n",
    "    \n",
    "MIDIDIR = \"C:\\\\Users\\\\Lukey\\\\Music_Project_Test\\\\Generated_MIDI\"\n",
    "WAVDIR = \"C:\\\\Users\\\\Lukey\\\\Music_Project_Test\\\\Generated_Audio\"\n",
    "SFDIR = \"C:\\\\Users\\\\Lukey\\\\Music_Project_Test\\\\soundfont.sf2\"\n",
    "\n",
    "ss2_generation(MIDIDIR, WAVDIR, SFDIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
